{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**\n",
    "\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/guides/text\n",
    "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a lively urban street scene with a mix of pedestrians and vehicles. In the foreground, there is a\n",
      "wooden bench on a city sidewalk where an elderly man in a suit is sitting, and next to him, a young woman is reading a\n",
      "newspaper. Nearby, a young boy is seated on the ground, focused on a tablet or smartphone, and another person is lying\n",
      "on the pavement, dressed in casual clothing including a red jacket and jeans.  Several pigeons are scattered around this\n",
      "area. In the street crossing, a man riding a bicycle, a person on a scooter, and a musician playing a guitar are\n",
      "visible. Cars and a taxi are driving through the intersection. Buildings with glass windows and storefronts line the\n",
      "street, and the scene appears to be set during the day, with sunlight illuminating the area and skyscrapers in the\n",
      "background. There are also a few people walking on the sidewalk at the right side of the image. The overall atmosphere\n",
      "feels dynamic and vibrant.\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt with prompt and image\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the json in a dict structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json. loads() converts JSON strings to Python objects\n",
    "output = json.loads(returnValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can access specific infos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene': 'Urban city street intersection during daytime',\n",
       " 'background': {'buildings': [{'style': 'Brick facade with green awnings',\n",
       "    'windows': 'Large, reflecting sunlight',\n",
       "    'shops': 'Visible with warm lighting inside'},\n",
       "   {'style': 'Modern glass skyscrapers',\n",
       "    'height': 'Very tall, extending into the sky',\n",
       "    'details': 'Reflective surfaces, some with trees in front'},\n",
       "   {'church': {'architecture': 'Traditional with a steeple',\n",
       "     'location': 'Center background, between skyscrapers'}}],\n",
       "  'traffic_light': {'color': 'Yellow',\n",
       "   'position': 'Hanging over the intersection'}},\n",
       " 'street': {'crosswalk': 'Wide zebra stripes, busy with activity',\n",
       "  'vehicles': [{'type': 'Taxi',\n",
       "    'color': 'White',\n",
       "    'motion': 'Blurred, indicating movement'},\n",
       "   {'type': 'SUV', 'color': 'Gray', 'motion': 'Moving slowly'},\n",
       "   {'type': 'Sedan', 'color': 'Orange', 'motion': 'Blurred, moving fast'}],\n",
       "  'motorcycles': [{'rider': 'Wearing black gear and white helmet',\n",
       "    'motion': 'Moving across crosswalk'},\n",
       "   {'rider': 'Wearing casual clothes and helmet',\n",
       "    'vehicle': 'Scooter',\n",
       "    'motion': 'Moving slowly'}]},\n",
       " 'people': [{'position': 'Sitting on sidewalk near crosswalk',\n",
       "   'activity': 'Using a tablet',\n",
       "   'clothing': 'Green jacket, shorts, sneakers'},\n",
       "  {'position': 'Lying on sidewalk',\n",
       "   'clothing': 'Red hoodie, blue jeans, black sneakers',\n",
       "   'posture': 'Relaxed, looking upwards'},\n",
       "  {'position': 'Sitting on wooden bench',\n",
       "   'activity': 'Reading a newspaper',\n",
       "   'clothing': 'Red striped blouse, blue jeans, black shoes'},\n",
       "  {'position': 'Sitting on wooden bench',\n",
       "   'activity': 'Thinking or resting with hand on face',\n",
       "   'clothing': 'Gray suit, glasses'},\n",
       "  {'position': 'Walking on sidewalk',\n",
       "   'activity': 'Looking at phone',\n",
       "   'clothing': 'Pink top, denim shorts, white sneakers'},\n",
       "  {'position': 'Crossing street',\n",
       "   'activity': 'Playing guitar while walking',\n",
       "   'clothing': 'Black jacket, black pants, black cap'}],\n",
       " 'animals': {'pigeons': {'count': 7,\n",
       "   'location': 'On sidewalk and near people',\n",
       "   'behavior': 'Pecking or standing'}},\n",
       " 'plants': {'flower_pot': {'location': 'Near person sitting on sidewalk',\n",
       "   'flowers': 'Red blossoms with green leaves'},\n",
       "  'trees': {'location': 'Along sidewalks and street edges',\n",
       "   'appearance': 'Green and leafy'}},\n",
       " 'lighting': {'time_of_day': 'Late afternoon or early evening',\n",
       "  'sunlight': 'Soft, casting long shadows',\n",
       "  'overall_ambiance': 'Warm and lively city atmosphere'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n",
    "#output[\"foreground\"][\"people\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",    \n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_extraction = json.loads(returnValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'sitting on the ground near a flower pot',\n",
       "  'age': 16,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'male'},\n",
       " {'position': 'lying on the ground',\n",
       "  'age': 18,\n",
       "  'activity': 'resting or sleeping',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 65,\n",
       "  'activity': 'reading a newspaper',\n",
       "  'gender': 'female'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 70,\n",
       "  'activity': 'thinking or resting with hand on face',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking on the sidewalk near the bench',\n",
       "  'age': 20,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'female'},\n",
       " {'position': 'riding a motorcycle in the crosswalk',\n",
       "  'age': 30,\n",
       "  'activity': 'riding motorcycle',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking in the crosswalk playing guitar',\n",
       "  'age': 25,\n",
       "  'activity': 'playing guitar',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a scooter in the crosswalk',\n",
       "  'age': 28,\n",
       "  'activity': 'riding scooter',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_extraction[\"people\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: \n",
    "\n",
    "\n",
    "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str \n",
    "    age: int \n",
    "    activity: str \n",
    "    gender: str\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int \n",
    "    atmosphere: str \n",
    "    hour_of_the_day: int \n",
    "    people: list[Person] \n",
    "\n",
    "completion = openAIclient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
    "    ],\n",
    "    response_format=ImageExtraction,\n",
    ")\n",
    "\n",
    "output_image_extraction = completion.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one appears to be in immediate danger based on the scene. No hospital alert is necessary at this time.\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": alert_sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": alert_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest person in the picture, according to the provided list, is the 16-year-old male sitting on the ground near a\n",
      "flower pot using a smartphone.  Based on the image, his approximate box_2d coordinates normalized to 0-1000 are:  [ymin,\n",
      "xmin, ymax, xmax] = [755, 144, 930, 310]\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart\n",
    "- https://ai.google.dev/gemini-api/docs/text-generation\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine it like teaching a very fast student.  You show it *lots* of examples – like showing a child thousands of\n",
      "pictures of cats until it knows what a cat is.  Then, it can recognize new cats, understand what you say, or help answer\n",
      "questions, all very quickly and cleverly.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"\n",
    ")\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image presents a bustling urban street scene, bathed in the warm, golden light of what appears to be late afternoon\n",
      "or early morning. The perspective is eye-level, looking down a long city street, creating a strong sense of depth.\n",
      "**Foreground:** The immediate foreground features a concrete sidewalk and a prominent zebra crosswalk. *   To the far\n",
      "left, a young person with short brown hair sits cross-legged on the pavement, engrossed in a tablet or phone. They wear\n",
      "a green jacket and grey shorts. Beside them, a rustic wooden pot overflows with vibrant red geraniums. *   Closer to the\n",
      "center, another young individual, dressed in a red hoodie and blue jeans, lies sprawled casually on their back on the\n",
      "sidewalk, eyes closed. *   Numerous pigeons are scattered across the sidewalk and crosswalk, some pecking at the ground,\n",
      "others standing still. *   To the right, a classic wooden park bench provides a moment of respite for two individuals.\n",
      "An older man in a dark suit sits on the left of the bench, his hand thoughtfully touching his chin, perhaps reading or\n",
      "contemplating. Next to him, a woman with blonde hair, wearing a striped red top and blue jeans, is absorbed in reading a\n",
      "newspaper.  **Midground (Crosswalk & Street):** The crosswalk itself is a hub of dynamic motion. *   A silver sedan,\n",
      "possibly a taxi or ride-share due to a light-colored sign on its roof, speeds across the crosswalk from right to left,\n",
      "blurred to indicate motion. *   Behind it, an orange SUV moves from left to right, also motion-blurred. *   Several\n",
      "people are actively crossing the street:     *   A motorcyclist in a black leather suit and white helmet rides a\n",
      "vintage-style motorcycle from left to right.     *   A street performer or musician strides across the crosswalk,\n",
      "playing an acoustic guitar. He wears a dark jacket, trousers, and a cap.     *   Behind the musician, a woman on a\n",
      "scooter, wearing a brown jacket, moves across the street. *   On the right-hand sidewalk, a young woman walks casually\n",
      "towards the viewer, holding a small tray or plate of food. She wears a pink t-shirt and light blue denim shorts, and has\n",
      "long dark hair. Behind her, a silver SUV is parked or moving slowly.  **Background (Cityscape):** The background reveals\n",
      "a diverse cityscape under a bright, slightly hazy sky with a warm glow. *   To the left, traditional brick buildings\n",
      "with ornate arched windows and green awnings line the street. Warm light spills from their windows, suggesting shops or\n",
      "cafes. *   In the center-background, modern glass skyscrapers pierce the sky, sleek and towering. Amidst them, a\n",
      "striking older building with a pointed spire, possibly a church, provides an architectural contrast. *   To the right, a\n",
      "long street stretches into the distance, lined with a mix of contemporary and classic architecture. A traffic light\n",
      "signal, hanging above the street, shows a red light. *   Trees with green foliage are visible along the sidewalks,\n",
      "adding touches of nature to the urban environment.  The scene captures a lively urban tapestry, where daily commutes,\n",
      "street performances, quiet contemplation, and unexpected moments coexist under a warm, diffused light, giving the\n",
      "impression of a vibrant, living city.\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
    "                                          )\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [197, 600, 359, 891],\n",
      "    \"rot_y\": -0.2,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": -0.2,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [320, 700, 627, 966],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [321, 291, 500, 477],\n",
      "    \"rot_y\": 0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"motorcycle\",\n",
      "    \"box_2d\": [335, 389, 550, 573],\n",
      "    \"rot_y\": 0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [498, 315, 612, 532],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"guitar\",\n",
      "    \"box_2d\": [539, 395, 630, 467],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [610, 316, 715, 463],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"scooter\",\n",
      "    \"box_2d\": [628, 380, 743, 513],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [577, 393, 726, 751],\n",
      "    \"rot_y\": 0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [669, 482, 856, 828],\n",
      "    \"rot_y\": -0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": -0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"newspaper\",\n",
      "    \"box_2d\": [738, 584, 803, 679],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [840, 278, 1000, 650],\n",
      "    \"rot_y\": -0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": true,\n",
      "    \"alpha\": -0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [0, 312, 441, 608],\n",
      "    \"rot_y\": -0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": true,\n",
      "    \"alpha\": -0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [391, 321, 550, 442],\n",
      "    \"rot_y\": -0.1,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": -0.1,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [194, 331, 450, 469],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"car\",\n",
      "    \"box_2d\": [609, 396, 946, 533],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"traffic light\",\n",
      "    \"box_2d\": [657, 32, 737, 137],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"bench\",\n",
      "    \"box_2d\": [593, 528, 999, 909],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"flower pot\",\n",
      "    \"box_2d\": [110, 769, 240, 929],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [368, 582, 432, 638],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [492, 573, 545, 626],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [500, 625, 555, 672],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [462, 677, 517, 725],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [283, 642, 335, 693],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [807, 627, 882, 687],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"pigeon\",\n",
      "    \"box_2d\": [635, 831, 715, 902],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": false,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [221, 401, 290, 442],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [288, 381, 355, 417],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [328, 362, 380, 392],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [704, 305, 770, 360],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [742, 293, 792, 332],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [780, 290, 830, 325],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [775, 308, 838, 349],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"person\",\n",
      "    \"box_2d\": [804, 296, 856, 337],\n",
      "    \"rot_y\": 0.0,\n",
      "    \"occluded\": true,\n",
      "    \"truncated\": false,\n",
      "    \"alpha\": 0.0,\n",
      "    \"score\": 1.0\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
    "\n",
    "\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it match your schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'box_2d': [665, 412, 937, 517]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
    "\n",
    "config={\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[img, prompt],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "bounding_boxes = json.loads(response.text)\n",
    "print(bounding_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
    "\n",
    "Consider the file: `/images/prescription1.jpg`.  \n",
    "Have a look at it.\n",
    "\n",
    "### JSON Schema\n",
    "Let’s define a JSON schema for the extraction task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_prescription = {\n",
    " \"name\": \"prescription_extract\",\n",
    "\"schema\": {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"doctor_name\": { \"type\": \"string\" },\n",
    "    \"patient_name\": { \"type\": \"string\" },\n",
    "    \"patient_dob\": { \"type\": \"string\" },\n",
    "    \"meds\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": { \"type\": \"string\" },\n",
    "          \"dose\": { \"type\": \"string\" },\n",
    "          \"frequency\": { \"type\": \"string\" },\n",
    "          \"instructions\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "      }\n",
    "    },\n",
    "    \"signature\": { \"type\": \"boolean\" }\n",
    "  },\n",
    "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract structured infos using Gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doctor\": \"Dr. Markus Müller\",\n",
      "  \"patient\": \"Claudie Fischer\",\n",
      "  \"date_of_birth\": \"1978-04-01\",\n",
      "  \"gender\": \"Female\",\n",
      "  \"medication\": \"Ibuprofen\",\n",
      "  \"dosage\": \"400mg\",\n",
      "  \"frequency\": \"3 times daily\",\n",
      "  \"instructions\": \"after eating\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"images/prescription1.jpg\")\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema_prescription,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Extract infos from image, following the given json schema.\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
    "Below is an example helper function that does this.\n",
    "\n",
    "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
    "> We avoid that here to keep the workflow generally compatible across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "def parse_json_in_output(output):\n",
    "    \"\"\"\n",
    "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The text output containing the JSON data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    # Regex to extract JSON-like portion\n",
    "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        # Fix single quotes and ensure proper JSON formatting\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        try:\n",
    "            # Convert the fixed JSON string into a dictionary\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            return \"The extracted JSON is still not valid after formatting.\"\n",
    "    else:\n",
    "        return \"No JSON data found in the given output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor': 'Dr. Markus Müller', 'patient': 'Claudie Fischer', 'date_of_birth': '1978-04-01', 'gender': 'Female', 'medication': 'Ibuprofen', 'dosage': '400mg', 'frequency': '3 times daily', 'instructions': 'after eating'}\n"
     ]
    }
   ],
   "source": [
    "print(parse_json_in_output(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doctor': 'Dr. Markus Müller',\n",
       " 'patient': 'Claudie Fischer',\n",
       " 'date_of_birth': '1978-04-01',\n",
       " 'gender': 'Female',\n",
       " 'medication': 'Ibuprofen',\n",
       " 'dosage': '400mg',\n",
       " 'frequency': '3 times daily',\n",
       " 'instructions': 'after eating'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = \"images/prescription1.jpg\"\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference wiht the output of Gemini vs your schema? \n",
    "\n",
    "No need for parsing now. We load the json in a python dict structure with json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(returnValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
