{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**\n",
    "\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/guides/text\n",
    "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a busy urban street scene with a mix of activities and elements. In the foreground, there are people\n",
      "engaging in various activities on the sidewalk and a bench. One person is sitting on the ground reading a book or\n",
      "looking at a device, another person is lying on the ground wearing a red jacket and a face mask. Two individuals are\n",
      "seated on a bench, one reading a newspaper and the other appearing thoughtful. There are several pigeons around them.\n",
      "In the background, there are vehicles including a taxi, a motorbike, a person on a bicycle, and a scooter moving along\n",
      "the street. A person playing a guitar is walking in the middle of the crosswalk. The scene features tall buildings,\n",
      "trees, and a traffic signal. The lighting suggests it is either early morning or late afternoon with sunlight\n",
      "illuminating the cityscape. The overall atmosphere appears lively and dynamic.\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt with prompt and image\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the json in a dict structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json. loads() converts JSON strings to Python objects\n",
    "output = json.loads(returnValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can access specific infos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene': 'Urban city street with a mix of pedestrians, vehicles, and buildings',\n",
       " 'time_of_day': 'Late afternoon or early evening, with sunlight casting long shadows',\n",
       " 'background': {'buildings': [{'style': 'Modern glass skyscrapers',\n",
       "    'location': 'Far background'},\n",
       "   {'style': 'Older brick buildings with green awnings',\n",
       "    'location': 'Left side of the street'},\n",
       "   {'style': 'Mixed modern and older buildings',\n",
       "    'location': 'Right side of the street'},\n",
       "   {'special_building': 'Church with a steeple',\n",
       "    'location': 'Center background'}],\n",
       "  'traffic_lights': {'color': 'Yellow',\n",
       "   'position': 'Above the street on the right side'}},\n",
       " 'street': {'crosswalk': 'Wide zebra crossing with white stripes',\n",
       "  'vehicles': [{'type': 'Taxi',\n",
       "    'color': 'White',\n",
       "    'motion': 'Blurred, moving left to right'},\n",
       "   {'type': 'SUV', 'color': 'Gray', 'motion': 'Moving left to right'},\n",
       "   {'type': 'Sedan', 'color': 'Orange', 'motion': 'Moving left to right'}],\n",
       "  'motorcycles': [{'rider': 'Wearing black gear and white helmet',\n",
       "    'motion': 'Moving across the crosswalk'},\n",
       "   {'rider': 'Wearing casual clothes and helmet',\n",
       "    'vehicle': 'Scooter',\n",
       "    'motion': 'Moving across the crosswalk'}],\n",
       "  'pedestrians': [{'activity': 'Playing guitar while walking',\n",
       "    'clothing': 'Black jacket and pants, black cap'}]},\n",
       " 'sidewalk': {'bench': {'material': 'Wooden slats with black metal frame',\n",
       "   'people': [{'age': 'Older man',\n",
       "     'clothing': 'Gray suit',\n",
       "     'activity': 'Sitting, hand on chin, thoughtful'},\n",
       "    {'age': 'Young woman',\n",
       "     'clothing': 'Red striped blouse, blue jeans',\n",
       "     'activity': 'Reading a newspaper'}]},\n",
       "  'people': [{'age': 'Young person',\n",
       "    'clothing': 'Green jacket, shorts',\n",
       "    'activity': 'Sitting on the ground using a tablet or phone'},\n",
       "   {'age': 'Young person',\n",
       "    'clothing': 'Red jacket, blue jeans',\n",
       "    'activity': 'Lying on the ground, eyes closed or resting'},\n",
       "   {'age': 'Young woman',\n",
       "    'clothing': 'Pink top, denim shorts',\n",
       "    'activity': 'Walking while looking at a phone'}],\n",
       "  'pigeons': {'number': 7,\n",
       "   'location': 'Scattered on the sidewalk near the bench and people'},\n",
       "  'plant': {'type': 'Flowering plant with red flowers',\n",
       "   'container': 'Brown pot',\n",
       "   'location': 'Near the person sitting on the ground with a tablet'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n",
    "#output[\"foreground\"][\"people\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",    \n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_extraction = json.loads(returnValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'sitting on the ground near a flower pot',\n",
       "  'age': 16,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'male'},\n",
       " {'position': 'lying on the ground',\n",
       "  'age': 18,\n",
       "  'activity': 'resting or sleeping',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 65,\n",
       "  'activity': 'thinking or resting',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 25,\n",
       "  'activity': 'reading a newspaper',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 20,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'female'},\n",
       " {'position': 'riding a motorcycle',\n",
       "  'age': 30,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking on the street',\n",
       "  'age': 28,\n",
       "  'activity': 'playing guitar',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a scooter',\n",
       "  'age': 27,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'female'},\n",
       " {'position': 'inside a taxi',\n",
       "  'age': 40,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'inside a taxi',\n",
       "  'age': 35,\n",
       "  'activity': 'passenger',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk in the background',\n",
       "  'age': 30,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_extraction[\"people\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: \n",
    "\n",
    "\n",
    "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str \n",
    "    age: int \n",
    "    activity: str \n",
    "    gender: str\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int \n",
    "    atmosphere: str \n",
    "    hour_of_the_day: int \n",
    "    people: list[Person] \n",
    "\n",
    "completion = openAIclient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
    "    ],\n",
    "    response_format=ImageExtraction,\n",
    ")\n",
    "\n",
    "output_image_extraction = completion.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one appears to be in immediate danger in the scene described. Therefore, there is no need to alert either a Child\n",
      "Hospital or a normal Hospital.\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": alert_sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": alert_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest person in the list you provided is the 16-year-old male sitting on the ground near a flower pot using a\n",
      "smartphone.  Looking at the image, this corresponds to the boy sitting on the ground in the front left area of the\n",
      "image, interacting with a smartphone or similar device.  The approximate normalized coordinates of the bounding box\n",
      "around this person in the format [ymin, xmin, ymax, xmax] scaled to 0-1000 are:  [650, 110, 900, 300]\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart\n",
    "- https://ai.google.dev/gemini-api/docs/text-generation\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client()\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is like a very clever computer helper. It learns from tons of examples, just like we learn from experience. Then it\n",
      "uses that knowledge to figure things out, answer questions, or help with tasks.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"\n",
    ")\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image captures a vibrant and bustling city street scene, bathed in the warm, golden light of what appears to be\n",
      "late afternoon or early morning. The perspective is eye-level, placing the viewer amidst the street-level activity and\n",
      "the prominent zebra crossing.  In the **background**, a dynamic urban landscape unfolds. On the left, older, multi-story\n",
      "brick buildings with numerous windows and green awnings over shopfronts suggest historical architecture. As one looks\n",
      "further into the distance and to the right, modern glass skyscrapers rise alongside more traditional structures,\n",
      "including what looks like a church spire, creating a striking contrast between old and new. The sky above is bright and\n",
      "hazy, filtering the sunlight.  The **midground** is characterized by motion. Cars are moving across the street, depicted\n",
      "with significant motion blur to convey speed. A silver sedan, possibly a taxi given the blurred yellow sign on its roof,\n",
      "streaks across the left side of the crosswalk. Further back, a grey SUV and a golden-orange car are also in motion.\n",
      "Dominating the center of the crosswalk, a man wearing a black leather suit and a white helmet rides a motorcycle, his\n",
      "figure slightly blurred. To his right, another man, dressed in dark clothing, walks across the crosswalk while playing\n",
      "an acoustic guitar, seemingly absorbed in his music. Further right, a woman in a hat rides a white scooter or moped,\n",
      "also in motion. Along the sidewalk on the right, several other pedestrians are visible in the distance, though blurred.\n",
      "The **foreground** is rich with detail and stationary figures. A prominent black and white zebra crossing spans the\n",
      "width of the street. On the right side of the frame, a long, slatted wooden bench with black metal arms is occupied by\n",
      "two people. An older man in a dark suit and glasses sits on the left side of the bench, resting his chin on his hand in\n",
      "a contemplative pose. Next to him, a woman with blonde hair, wearing a red striped top and blue jeans, is engrossed in\n",
      "reading a newspaper or magazine. Pigeons are scattered on the sidewalk around the bench, some walking, some standing.\n",
      "Further to the right, a young woman with long dark hair, wearing a pink t-shirt and denim shorts, walks past the bench,\n",
      "holding what appears to be a small plate or tray. Above the street on the right, a traffic light displays three yellow\n",
      "signals, hanging from a metal arm.  On the left side of the foreground, next to the crosswalk, a large wooden planter\n",
      "overflows with vibrant red flowers, possibly geraniums. Adjacent to this, a young boy with short brown hair sits cross-\n",
      "legged on the sidewalk, intently looking at a tablet or smartphone in his hands. Most notably, a young man in a red\n",
      "hoodie and blue jeans lies completely flat on his back on the sidewalk between the flower pot and the pigeons, appearing\n",
      "relaxed and perhaps gazing upward, seemingly oblivious to the surrounding hustle.  Overall, the scene captures a\n",
      "snapshot of urban life, blending motion and stillness, modern and historic elements, and a diverse range of human\n",
      "activities and interactions within a warmly lit city environment.\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
    "                                          )\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scene_description\": \"A dynamic and bustling urban street scene during the golden hour, featuring a mix of pedestrians, vehicles, and towering city buildings. The warm, low-angle sunlight casts long shadows and highlights the architectural details and the diverse activities of people on the sidewalk and crossing the street.\",\n",
      "  \"main_elements\": [\n",
      "    \"people\",\n",
      "    \"vehicles\",\n",
      "    \"buildings\",\n",
      "    \"street_furniture\",\n",
      "    \"natural_elements\"\n",
      "  ],\n",
      "  \"location\": \"A city street with a prominent zebra crossing, flanked by multi-story buildings and distant skyscrapers.\",\n",
      "  \"time_of_day\": \"Late afternoon or early evening, characterized by warm, golden sunlight and long shadows.\",\n",
      "  \"mood_and_atmosphere\": \"Lively, active, and urban, with a sense of continuous motion and everyday city life.\",\n",
      "  \"lighting_and_colors\": \"Warm, golden lighting from a low sun, creating a vibrant glow on buildings and the street. Colors are varied, including brick reds, greys, greens, and blues, with striking contrasts between sunlit and shadowed areas.\",\n",
      "  \"composition\": \"The scene is captured at eye-level, with a strong diagonal emphasis created by the zebra crossing, leading the eye towards the bustling street and the distant cityscape. Various elements are distributed across the frame, creating a sense of depth and activity.\",\n",
      "  \"detailed_elements\": [\n",
      "    \"A silver sedan with a yellow taxi sign on its roof, blurred by motion, moving from right to left across the zebra crossing in the foreground.\",\n",
      "    \"A person wearing a black leather jacket and a white helmet, riding a black motorcycle from left to right across the zebra crossing.\",\n",
      "    \"A man in dark attire, walking from left to right across the zebra crossing while playing a brown acoustic guitar.\",\n",
      "    \"A woman on a light-colored scooter, dressed in dark clothing, riding in the mid-ground.\",\n",
      "    \"A young man lying on his back on the sidewalk in the bottom-center, wearing a red hoodie and blue jeans, with pigeons scattered nearby.\",\n",
      "    \"A young man sitting cross-legged on the sidewalk in the bottom-left, wearing an olive-green jacket and grey shorts, looking at a white tablet.\",\n",
      "    \"An elderly man in a grey suit and white shirt, wearing glasses, sitting on a wooden bench, with his right hand to his chin, seemingly contemplating.\",\n",
      "    \"A woman with blonde hair, wearing a striped red and orange long-sleeve top and blue jeans, sitting on the same wooden bench, reading a newspaper.\",\n",
      "    \"A young woman standing and walking to the right in the mid-ground, wearing a pink and red t-shirt, light blue denim shorts, and white sneakers, carrying a white plate or tray.\",\n",
      "    \"A light silver SUV moving in the mid-ground, behind the motorcycle.\",\n",
      "    \"An orange car blurred by motion, moving behind the motorcycle and SUV.\",\n",
      "    \"Multi-story brick buildings with numerous windows and ground-floor shops with green awnings on the left side of the street.\",\n",
      "    \"Tall, modern glass skyscrapers dominating the background skyline, interspersed with a darker, spired church-like building.\",\n",
      "    \"A brick and grey building on the right side with 'DELI & GRILLED CHEESE' visible on a sign.\",\n",
      "    \"Several pigeons are scattered on the sidewalk near the sitting and lying individuals, and near the bench.\",\n",
      "    \"A large wooden barrel pot with vibrant red flowers placed on the sidewalk in the bottom-left.\",\n",
      "    \"Traffic lights with illuminated yellow signals hanging above the street.\",\n",
      "    \"A striped zebra crossing extending across the street in the foreground and mid-ground.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
    "\n",
    "\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it match your schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'youngest', 'box_2d': [461, 461, 563, 529]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
    "\n",
    "config={\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[img, prompt],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "bounding_boxes = json.loads(response.text)\n",
    "print(bounding_boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
    "\n",
    "Consider the file: `/images/prescription1.jpg`.  \n",
    "Have a look at it.\n",
    "\n",
    "### JSON Schema\n",
    "Let’s define a JSON schema for the extraction task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_prescription = {\n",
    " \"name\": \"prescription_extract\",\n",
    "\"schema\": {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"doctor_name\": { \"type\": \"string\" },\n",
    "    \"patient_name\": { \"type\": \"string\" },\n",
    "    \"patient_dob\": { \"type\": \"string\" },\n",
    "    \"meds\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": { \"type\": \"string\" },\n",
    "          \"dose\": { \"type\": \"string\" },\n",
    "          \"frequency\": { \"type\": \"string\" },\n",
    "          \"instructions\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "      }\n",
    "    },\n",
    "    \"signature\": { \"type\": \"boolean\" }\n",
    "  },\n",
    "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract structured infos using Gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"images/prescription1.jpg\")\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema_prescription,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Extract infos from image, following the given json schema.\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
    "Below is an example helper function that does this.\n",
    "\n",
    "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
    "> We avoid that here to keep the workflow generally compatible across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "def parse_json_in_output(output):\n",
    "    \"\"\"\n",
    "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The text output containing the JSON data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    # Regex to extract JSON-like portion\n",
    "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        # Fix single quotes and ensure proper JSON formatting\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        try:\n",
    "            # Convert the fixed JSON string into a dictionary\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            return \"The extracted JSON is still not valid after formatting.\"\n",
    "    else:\n",
    "        return \"No JSON data found in the given output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(parse_json_in_output(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = \"images/prescription1.jpg\"\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference wiht the output of Gemini vs your schema? \n",
    "\n",
    "No need for parsing now. We load the json in a python dict structure with json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(returnValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
