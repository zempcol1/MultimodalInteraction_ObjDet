{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  - YOLO & OWL-ViT\n",
    "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
    "\n",
    "- Running object detection inference on images/videos\n",
    "- Fine-tuning YOLO for custom datasets\n",
    "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Object Detection Inference\n",
    "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
    "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Inference**: The process of using a trained model to make predictions on new data.\n",
    "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the YOLOv8 model using the Ultralytics library.\n",
    "2. Perform inference on a sample image to detect objects.\n",
    "3. Visualize the results, including bounding boxes and class labels.\n",
    "\n",
    "**Support Material:**\n",
    "- https://docs.ultralytics.com/models/yolov8/\n",
    "- https://docs.ultralytics.com/tasks/detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/street_scene.jpg: 384x640 13 persons, 1 bicycle, 9 cars, 2 motorcycles, 1 traffic light, 1 bench, 4 birds, 1 handbag, 1 potted plant, 165.7ms\n",
      "Speed: 3.2ms preprocess, 165.7ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 2.,  0.,  0.,  0., 58.,  0.,  2.,  9.,  0., 14.,  0.,  3.,  0.,  1.,  2., 14., 14.,  2.,  0.,  2.,  2.,  0.,  0., 26.,  0.,  3.,  2.,  0.,  0.,  2., 14., 13.,  2.])\n",
      "conf: tensor([0.9098, 0.9041, 0.9005, 0.8934, 0.8477, 0.8331, 0.8173, 0.7737, 0.7585, 0.7313, 0.6779, 0.6606, 0.6198, 0.5686, 0.5105, 0.5057, 0.5043, 0.4675, 0.4564, 0.4517, 0.4201, 0.4165, 0.4037, 0.4015, 0.3767, 0.3745, 0.3659, 0.3221, 0.3095, 0.3049, 0.2999, 0.2989, 0.2811])\n",
      "data: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02, 9.0984e-01, 2.0000e+00],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02, 9.0414e-01, 0.0000e+00],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02, 9.0047e-01, 0.0000e+00],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02, 8.9344e-01, 0.0000e+00],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02, 8.4768e-01, 5.8000e+01],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02, 8.3306e-01, 0.0000e+00],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02, 8.1727e-01, 2.0000e+00],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02, 7.7372e-01, 9.0000e+00],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02, 7.5851e-01, 0.0000e+00],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03, 7.3131e-01, 1.4000e+01],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02, 6.7793e-01, 0.0000e+00],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02, 6.6059e-01, 3.0000e+00],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02, 6.1981e-01, 0.0000e+00],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7393e+02, 5.6856e-01, 1.0000e+00],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02, 5.1050e-01, 2.0000e+00],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02, 5.0570e-01, 1.4000e+01],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02, 5.0429e-01, 1.4000e+01],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02, 4.6751e-01, 2.0000e+00],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02, 4.5638e-01, 0.0000e+00],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02, 4.5173e-01, 2.0000e+00],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02, 4.2015e-01, 2.0000e+00],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02, 4.1646e-01, 0.0000e+00],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02, 4.0370e-01, 0.0000e+00],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02, 4.0154e-01, 2.6000e+01],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02, 3.7673e-01, 0.0000e+00],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02, 3.7447e-01, 3.0000e+00],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02, 3.6586e-01, 2.0000e+00],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02, 3.2208e-01, 0.0000e+00],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02, 3.0946e-01, 0.0000e+00],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02, 3.0489e-01, 2.0000e+00],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02, 2.9992e-01, 1.4000e+01],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03, 2.9888e-01, 1.3000e+01],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02, 2.8114e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1024, 1792)\n",
      "shape: torch.Size([33, 6])\n",
      "xywh: tensor([[ 303.4360,  491.6105,  604.9601,  254.6311],\n",
      "        [1329.7615,  644.0153,  301.6866,  440.0984],\n",
      "        [1638.5154,  483.1179,  145.6616,  385.4641],\n",
      "        [1190.8245,  571.8881,  264.4564,  331.1922],\n",
      "        [ 300.1982,  827.6760,  220.0836,  259.5260],\n",
      "        [ 533.2831,  754.9047,  236.2436,  343.7872],\n",
      "        [ 494.3851,  373.8213,  242.3596,   93.7392],\n",
      "        [1197.0276,   95.0568,   42.4672,  112.3248],\n",
      "        [ 986.5753,  428.2856,  106.6277,  231.5882],\n",
      "        [1234.8212,  975.6396,  117.0117,   93.0833],\n",
      "        [ 862.1802,  854.5883,  539.6152,  280.3784],\n",
      "        [1122.5028,  441.1805,  128.7488,  133.3917],\n",
      "        [ 801.0178,  412.6851,  120.5370,  222.7268],\n",
      "        [ 792.1692,  493.4145,  184.0449,  161.0408],\n",
      "        [ 687.8665,  405.7602,  215.7759,  120.7179],\n",
      "        [1191.9240,  895.1294,  112.9792,   93.6048],\n",
      "        [ 975.3347,  646.0970,   41.0160,   49.8470],\n",
      "        [1715.0410,  429.7282,  153.3424,  126.2471],\n",
      "        [1368.0439,  390.0021,   35.7831,   82.3866],\n",
      "        [ 732.8855,  401.7635,  306.5644,  120.2286],\n",
      "        [1743.0010,  430.4392,   97.0337,  125.0184],\n",
      "        [1456.9026,  394.0114,   30.0504,   96.6480],\n",
      "        [1131.1941,  403.8324,   97.5084,  157.3739],\n",
      "        [ 965.3071,  412.3028,   65.9980,   66.8322],\n",
      "        [1147.6650,  400.6788,   65.0867,  149.9766],\n",
      "        [ 793.3781,  488.7257,  183.1340,  171.8770],\n",
      "        [1668.7405,  423.7186,  246.1128,  133.3057],\n",
      "        [1360.7776,  387.9605,   35.5201,   85.3629],\n",
      "        [1449.9895,  396.8608,   34.6515,   92.0872],\n",
      "        [1577.1621,  431.4725,   87.7750,  108.1397],\n",
      "        [1042.2415,  629.9613,   29.6053,   42.7921],\n",
      "        [1484.4592,  815.0737,  482.9518,  407.9140],\n",
      "        [ 902.7610,  369.6018,   74.6650,   39.0404]])\n",
      "xywhn: tensor([[0.1693, 0.4801, 0.3376, 0.2487],\n",
      "        [0.7421, 0.6289, 0.1684, 0.4298],\n",
      "        [0.9144, 0.4718, 0.0813, 0.3764],\n",
      "        [0.6645, 0.5585, 0.1476, 0.3234],\n",
      "        [0.1675, 0.8083, 0.1228, 0.2534],\n",
      "        [0.2976, 0.7372, 0.1318, 0.3357],\n",
      "        [0.2759, 0.3651, 0.1352, 0.0915],\n",
      "        [0.6680, 0.0928, 0.0237, 0.1097],\n",
      "        [0.5505, 0.4182, 0.0595, 0.2262],\n",
      "        [0.6891, 0.9528, 0.0653, 0.0909],\n",
      "        [0.4811, 0.8346, 0.3011, 0.2738],\n",
      "        [0.6264, 0.4308, 0.0718, 0.1303],\n",
      "        [0.4470, 0.4030, 0.0673, 0.2175],\n",
      "        [0.4421, 0.4819, 0.1027, 0.1573],\n",
      "        [0.3839, 0.3963, 0.1204, 0.1179],\n",
      "        [0.6651, 0.8741, 0.0630, 0.0914],\n",
      "        [0.5443, 0.6310, 0.0229, 0.0487],\n",
      "        [0.9571, 0.4197, 0.0856, 0.1233],\n",
      "        [0.7634, 0.3809, 0.0200, 0.0805],\n",
      "        [0.4090, 0.3923, 0.1711, 0.1174],\n",
      "        [0.9727, 0.4204, 0.0541, 0.1221],\n",
      "        [0.8130, 0.3848, 0.0168, 0.0944],\n",
      "        [0.6312, 0.3944, 0.0544, 0.1537],\n",
      "        [0.5387, 0.4026, 0.0368, 0.0653],\n",
      "        [0.6404, 0.3913, 0.0363, 0.1465],\n",
      "        [0.4427, 0.4773, 0.1022, 0.1678],\n",
      "        [0.9312, 0.4138, 0.1373, 0.1302],\n",
      "        [0.7594, 0.3789, 0.0198, 0.0834],\n",
      "        [0.8091, 0.3876, 0.0193, 0.0899],\n",
      "        [0.8801, 0.4214, 0.0490, 0.1056],\n",
      "        [0.5816, 0.6152, 0.0165, 0.0418],\n",
      "        [0.8284, 0.7960, 0.2695, 0.3984],\n",
      "        [0.5038, 0.3609, 0.0417, 0.0381]])\n",
      "xyxy: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7393e+02],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02]])\n",
      "xyxyn: tensor([[5.3344e-04, 3.5576e-01, 3.3812e-01, 6.0442e-01],\n",
      "        [6.5788e-01, 4.1403e-01, 8.2623e-01, 8.4381e-01],\n",
      "        [8.7371e-01, 2.8358e-01, 9.5499e-01, 6.6001e-01],\n",
      "        [5.9073e-01, 3.9677e-01, 7.3831e-01, 7.2020e-01],\n",
      "        [1.0611e-01, 6.8156e-01, 2.2893e-01, 9.3500e-01],\n",
      "        [2.3167e-01, 5.6935e-01, 3.6351e-01, 9.0508e-01],\n",
      "        [2.0826e-01, 3.1929e-01, 3.4351e-01, 4.1083e-01],\n",
      "        [6.5614e-01, 3.7983e-02, 6.7983e-01, 1.4767e-01],\n",
      "        [5.2079e-01, 3.0517e-01, 5.8030e-01, 5.3133e-01],\n",
      "        [6.5643e-01, 9.0732e-01, 7.2172e-01, 9.9822e-01],\n",
      "        [3.3057e-01, 6.9766e-01, 6.3169e-01, 9.7146e-01],\n",
      "        [5.9047e-01, 3.6571e-01, 6.6232e-01, 4.9597e-01],\n",
      "        [4.1336e-01, 2.9426e-01, 4.8063e-01, 5.1177e-01],\n",
      "        [3.9071e-01, 4.0322e-01, 4.9341e-01, 5.6048e-01],\n",
      "        [3.2365e-01, 3.3731e-01, 4.4406e-01, 4.5519e-01],\n",
      "        [6.3361e-01, 8.2844e-01, 6.9666e-01, 9.1986e-01],\n",
      "        [5.3283e-01, 6.0661e-01, 5.5572e-01, 6.5529e-01],\n",
      "        [9.1427e-01, 3.5801e-01, 9.9984e-01, 4.8130e-01],\n",
      "        [7.5343e-01, 3.4063e-01, 7.7340e-01, 4.2109e-01],\n",
      "        [3.2344e-01, 3.3364e-01, 4.9451e-01, 4.5105e-01],\n",
      "        [9.4558e-01, 3.5931e-01, 9.9973e-01, 4.8139e-01],\n",
      "        [8.0462e-01, 3.3759e-01, 8.2139e-01, 4.3197e-01],\n",
      "        [6.0404e-01, 3.1752e-01, 6.5845e-01, 4.7121e-01],\n",
      "        [5.2026e-01, 3.7001e-01, 5.5709e-01, 4.3527e-01],\n",
      "        [6.2228e-01, 3.1806e-01, 6.5860e-01, 4.6452e-01],\n",
      "        [3.9164e-01, 3.9335e-01, 4.9383e-01, 5.6120e-01],\n",
      "        [8.6255e-01, 3.4870e-01, 9.9989e-01, 4.7888e-01],\n",
      "        [7.4945e-01, 3.3719e-01, 7.6927e-01, 4.2055e-01],\n",
      "        [7.9948e-01, 3.4259e-01, 8.1881e-01, 4.3252e-01],\n",
      "        [8.5562e-01, 3.6856e-01, 9.0460e-01, 4.7416e-01],\n",
      "        [5.7335e-01, 5.9430e-01, 5.8987e-01, 6.3609e-01],\n",
      "        [6.9363e-01, 5.9679e-01, 9.6313e-01, 9.9515e-01],\n",
      "        [4.8294e-01, 3.4188e-01, 5.2461e-01, 3.8000e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "results = model('images/street_scene.jpg', save = True)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning YOLO on Custom Dataset\n",
    "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
    "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
    "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.) Does it work? yes, no? why not? what can you do?\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
    "2. Configure the YOLO training pipeline.\n",
    "3. Train the model and evaluate its performance.\n",
    "\n",
    "**Support Material:** \n",
    "- https://docs.ultralytics.com/modes/train/\n",
    "- https://docs.ultralytics.com/modes/val/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace ./datasets/labels/train/Frame_100.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "# Download a sample dataset (e.g., Signature)\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
    "!unzip -q signature.zip -d ./datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.236 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.39 ðŸš€ Python-3.10.19 torch-2.9.1+cu128 CPU (AMD EPYC 7763 64-Core Processor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./datasets/signature.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 58/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/train.cache... 143 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<?, ?it/s]\n",
      "/home/vscode/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/val.cache... 35 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  0%|          | 0/9 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train YOLO on the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/signature.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/model.py:805\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py:388\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    384\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train YOLO on the dataset\n",
    "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/example_signature.jpg: 640x480 1 signature, 78.2ms\n",
      "Speed: 2.0ms preprocess, 78.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train/weights/best.pt\")  # load a custom model, check the path depending on your output before!!\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(\"images/example_signature.jpg\", conf=0.25) #check params if you need to improve detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Learning with OWL-ViT\n",
    "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
    "\n",
    "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
    "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
    "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
    "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
    "\n",
    "**Steps in Using OWL-ViT:**\n",
    "1. Model Initialization**: Set up the OWL-ViT model.\n",
    "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
    "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
    "\n",
    "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
    "\n",
    "**Support Material**:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected a church with confidence 0.116 at location [706.68, 35.65, 870.62, 342.14]\n",
      "Detected a person on the floor with confidence 0.119 at location [1265.73, 347.33, 1309.63, 423.22]\n",
      "Detected a person on the floor with confidence 0.144 at location [729.04, 301.7, 873.73, 509.65]\n",
      "Detected a person on the floor with confidence 0.109 at location [932.93, 316.24, 1046.45, 524.26]\n",
      "Detected a person on the floor with confidence 0.144 at location [1084.81, 326.57, 1179.79, 470.67]\n",
      "Detected a person on the floor with confidence 0.11 at location [1295.77, 343.81, 1340.44, 421.08]\n",
      "Detected a person on the floor with confidence 0.139 at location [1337.02, 344.97, 1389.21, 425.86]\n",
      "Detected a person on the floor with confidence 0.14 at location [1558.68, 294.65, 1722.22, 665.15]\n",
      "Detected a person on the floor with confidence 0.127 at location [1264.44, 347.31, 1313.74, 428.22]\n",
      "Detected a person on the floor with confidence 0.13 at location [1418.98, 349.44, 1474.32, 440.56]\n",
      "Detected a person on the floor with confidence 0.156 at location [1040.65, 408.02, 1329.82, 718.58]\n",
      "Detected a person on the floor with confidence 0.161 at location [1165.25, 425.62, 1484.56, 841.4]\n",
      "Detected a person on the floor with confidence 0.194 at location [408.87, 595.92, 658.65, 923.15]\n",
      "Detected a person on the floor with confidence 0.333 at location [584.38, 691.08, 1154.21, 1000.75]\n",
      "[706.68, 35.65, 870.62, 342.14]\n",
      "[1265.73, 347.33, 1309.63, 423.22]\n",
      "[729.04, 301.7, 873.73, 509.65]\n",
      "[932.93, 316.24, 1046.45, 524.26]\n",
      "[1084.81, 326.57, 1179.79, 470.67]\n",
      "[1295.77, 343.81, 1340.44, 421.08]\n",
      "[1337.02, 344.97, 1389.21, 425.86]\n",
      "[1558.68, 294.65, 1722.22, 665.15]\n",
      "[1264.44, 347.31, 1313.74, 428.22]\n",
      "[1418.98, 349.44, 1474.32, 440.56]\n",
      "[1040.65, 408.02, 1329.82, 718.58]\n",
      "[1165.25, 425.62, 1484.56, 841.4]\n",
      "[408.87, 595.92, 658.65, 923.15]\n",
      "[584.38, 691.08, 1154.21, 1000.75]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "\n",
    "text_labels = [[\"a person on the floor\", \"a church\"]]\n",
    "\n",
    "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(image.height, image.width)])\n",
    "\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")\n",
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    print(box)\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.savefig(\"streetscene_with_detections.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "show_boxes_and_labels_on_image(\n",
    "    image,\n",
    "    boxes,\n",
    "    text_labels,\n",
    "    scores\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
